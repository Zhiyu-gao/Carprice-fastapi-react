#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ‡‚è½¦å¸äºŒæ‰‹è½¦ çˆ¬è™«ï¼ˆåˆ—è¡¨é¡µ + è¯¦æƒ…é¡µ car-archives æœ€ç»ˆæ•´åˆç‰ˆï¼‰

- CDP æœ‰å¤´æµè§ˆå™¨
- åˆ—è¡¨é¡µï¼šå‘ç° car_id / title / image / tags
- è¯¦æƒ…é¡µï¼šè§£æ car-archives æ¡£æ¡ˆï¼ˆæ— å­—ä½“åçˆ¬ï¼‰
- æ¨èåˆ†å‰²çº¿ï¼šä¸Šé¢çˆ¬ï¼Œä¸‹é¢åœ
- æ–­ç‚¹ç»­çˆ¬ï¼šcar_id å·²å­˜åœ¨ç›´æ¥è·³è¿‡
"""

import json
import time
import random
import re
from pathlib import Path
from datetime import datetime
from urllib.parse import urljoin
import os
import requests
from playwright.sync_api import sync_playwright
from app.storage.local import save_image_local


# =========================
# é…ç½®åŒº
# =========================

CDP_ENDPOINT = "http://127.0.0.1:9321"

BASE_URL = "https://www.dongchedi.com"
LIST_URL_TEMPLATE = (
    "https://www.dongchedi.com/usedcar/"
    "x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-110000-{}-x-x-x-x-x"
)

START_PAGE = 1
END_PAGE = 2

SLEEP_PER_PAGE = (2.5, 4.0)
SLEEP_DETAIL = (1.2, 2.0)

DATA_DIR = Path(os.getenv("DATA_DIR", "data")).resolve()

CRAWL_DIR = DATA_DIR / "crawl"
JSON_DIR = CRAWL_DIR / "json"
IMG_DIR = CRAWL_DIR / "images"

JSON_DIR.mkdir(parents=True, exist_ok=True)
IMG_DIR.mkdir(parents=True, exist_ok=True)

# =========================
# å·¥å…·å‡½æ•°
# =========================

def normalize_img_url(url: str | None) -> str | None:
    if not url:
        return None
    if url.startswith("//"):
        return "https:" + url
    return url


# def download_image(url: str, save_path: Path):
#     url = normalize_img_url(url)
#     if not url:
#         return

#     headers = {
#         "User-Agent": "Mozilla/5.0",
#         "Referer": "https://www.dongchedi.com/",
#     }
#     r = requests.get(url, headers=headers, timeout=20)
#     r.raise_for_status()
#     save_path.write_bytes(r.content)


# =========================
# æ¨èåˆ†å‰²çº¿åˆ¤æ–­
# =========================

def is_card_after_recommend(page, card):
    """
    åˆ¤æ–­ card æ˜¯å¦ä½äºâ€œä¸ºæ‚¨æ¨èå…¨å›½ä¼˜è´¨äºŒæ‰‹è½¦â€ä¹‹å
    """
    return page.evaluate(
        """
        (card) => {
          const h1 = Array.from(document.querySelectorAll('h1'))
            .find(el => (el.innerText || '').includes('ä¸ºæ‚¨æ¨èå…¨å›½ä¼˜è´¨äºŒæ‰‹è½¦'));
          if (!h1) return false;
          return !!(h1.compareDocumentPosition(card) & Node.DOCUMENT_POSITION_FOLLOWING);
        }
        """,
        card
    )

def parse_price_block(page) -> dict:
    """
    è§£æä»·æ ¼åŒºå—ï¼š
    - æ–°è½¦æŒ‡å¯¼ä»·
    - æ¯”æ–°è½¦çœ
    - è®¡ç®—ä¼˜æƒ åä»·æ ¼
    """
    price = {
        "price_new_car": None,
        "price_discount": None,
        "price_after_discount": None,
        "price_unit": "ä¸‡",
    }

    ps = page.locator("p").filter(
        has_text=re.compile("æ–°è½¦|æ¯”æ–°è½¦çœ|å”®ä»·")
    )
    count = ps.count()

    for i in range(count):
        text = ps.nth(i).inner_text().strip()

        # æ–°è½¦æŒ‡å¯¼ä»·ï¼š10.98ä¸‡
        if "æ–°è½¦æŒ‡å¯¼ä»·" in text:
            m = re.search(r"([\d.]+)\s*ä¸‡", text)
            if m:
                price["price_new_car"] = float(m.group(1))

        # æ¯”æ–°è½¦çœï¼š7.20ä¸‡
        elif "æ¯”æ–°è½¦çœ" in text or "çœ" in text:
            m = re.search(r"([\d.]+)\s*ä¸‡", text)
            if m:
                price["price_discount"] = float(m.group(1))

        # æœ‰äº›é¡µé¢æ˜¯ï¼šå”®ä»·ï¼š3.78ä¸‡
        elif "å”®ä»·" in text or "ä»·æ ¼" in text:
            m = re.search(r"([\d.]+)\s*ä¸‡", text)
            if m:
                price["price_after_discount"] = float(m.group(1))

    # å¦‚æœæ²¡ç›´æ¥ç»™æˆäº¤ä»·ï¼Œå°±è®¡ç®—
    if (
        price["price_after_discount"] is None
        and price["price_new_car"] is not None
        and price["price_discount"] is not None
    ):
        price["price_after_discount"] = round(
            price["price_new_car"] - price["price_discount"], 2
        )

    return price


# =========================
# è¯¦æƒ…é¡µè§£æï¼ˆæ ¸å¿ƒï¼‰
# =========================

def parse_car_archives(page) -> dict:
    """
    è§£æè¯¦æƒ…é¡µ car-archives æ¡£æ¡ˆåŒº
    """
    info = {}

    items = page.locator("div.car-archives_item__1Y2Vp")
    count = items.count()

    for i in range(count):
        item = items.nth(i)
        try:
            name = item.locator("p.car-archives_name__1QrJz").inner_text().strip()
            value = item.locator("p.car-archives_value__3YXEW").inner_text().strip()
            if name and value:
                info[name] = value
        except Exception:
            continue

    return info


# =========================
# ä¸»æµç¨‹
# =========================

def main():
    with sync_playwright() as p:
        print("[INFO] è¿æ¥ Chrome CDP")
        browser = p.chromium.connect_over_cdp(CDP_ENDPOINT)
        context = browser.contexts[0]
        page = context.new_page()

        for page_no in range(START_PAGE, END_PAGE + 1):
            list_url = LIST_URL_TEMPLATE.format(page_no)
            print(f"\n[PAGE] {list_url}")

            page.goto(list_url, timeout=30000)
            time.sleep(random.uniform(*SLEEP_PER_PAGE))

            cards = page.locator("a.usedcar-card_card__3vUrx")
            total = cards.count()
            print(f"[INFO] æœ¬é¡µå‘ç°å¡ç‰‡ {total}")

            scraped = 0
            skipped = 0

            for i in range(total):
                card = cards.nth(i)

                # æ¨èåˆ†å‰²çº¿
                try:
                    if is_card_after_recommend(page, card):
                        print("[STOP] è¿›å…¥æ¨èåŒºï¼Œåœæ­¢æœ¬é¡µ")
                        break
                except Exception:
                    pass

                href = card.get_attribute("href")
                if not href or not href.startswith("/usedcar/"):
                    continue

                car_id = href.split("/")[-1]
                json_path = JSON_DIR / f"{car_id}.json"

                # æ–­ç‚¹ç»­çˆ¬
                if json_path.exists():
                    skipped += 1
                    continue

                title = card.locator("dt p").inner_text().strip()
                img_url = card.locator("img").first.get_attribute("src")

                tags = []
                try:
                    tags = [
                        s.inner_text().strip()
                        for s in card.locator("dd").nth(1).locator("span").all()
                        if s.inner_text().strip()
                    ]
                except Exception:
                    pass

                # === è¿›å…¥è¯¦æƒ…é¡µ ===
                detail_url = urljoin(BASE_URL, href)
                page.goto(detail_url, timeout=30000)
                page.wait_for_load_state("domcontentloaded")
                time.sleep(random.uniform(*SLEEP_DETAIL))

                info = parse_car_archives(page)
                price_info = parse_price_block(page)

                # ğŸ”¥ æŠŠä»·æ ¼å­—æ®µå¡è¿› info
                info.update({
                    "æ–°è½¦æŒ‡å¯¼ä»·": price_info.get("price_new_car"),
                    "æ¯”æ–°è½¦çœ": price_info.get("price_discount"),
                    "å½“å‰å”®ä»·": price_info.get("price_after_discount"),
                    "ä»·æ ¼å•ä½": price_info.get("price_unit"),
                })

                image_path = None
                data = {
                    "car_id": car_id,
                    "title": title,
                    "tags": tags,
                    "image_url": img_url,
                    "info": info,
                    **price_info,
                    "source_url": detail_url,
                    "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "image_path": image_path,
                    "page_no": page_no,
                }

                if img_url:
                    try:
                        img_url = normalize_img_url(img_url)
                        r = requests.get(img_url, headers={
                            "User-Agent": "Mozilla/5.0",
                            "Referer": "https://www.dongchedi.com/",
                        }, timeout=20)
                        r.raise_for_status()

                        image_path = save_image_local(
                            image_bytes=r.content,
                            filename=f"{car_id}.jpg",
                        )
                        data["image_path"] = image_path

                    except Exception as e:
                        print(f"[IMG FAIL] {car_id}: {e}")

                # å†™ JSON
                json_path.write_text(
                    json.dumps(data, ensure_ascii=False, indent=2),
                    encoding="utf-8"
                )

                scraped += 1
                print(f"[OK] {car_id} | {title}")

                # å›åˆ°åˆ—è¡¨é¡µï¼ˆé‡è¦ï¼‰
                page.go_back()
                page.wait_for_load_state("domcontentloaded")
                time.sleep(0.6)

            print(f"[SUMMARY] page={page_no} scraped={scraped} skipped={skipped}")

        print("\n[DONE] å…¨éƒ¨å®Œæˆ")
        browser.close()


if __name__ == "__main__":
    main()
